{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HAS-model embeddings for entities in a KG\n",
    "The purpose of these will be for assessing entities' similarities to one another. This measure of similarity between entities will in turn be used to assess the distinctiveness of labels that have previously been created and filtered for this KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "#from gensim.models import Word2Vec\n",
    "from src import graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters\n",
    "\n",
    "**Embedding model parameters**  \n",
    "*undirected*: Treat graph as undirected  \n",
    "*number_walks*: Number of random walks to start at each node  \n",
    "*walk_length*: Length of random walk started at each node  \n",
    "*seed*: Seed for random walk generator\n",
    "*representation_size*: Number of latent dimensions to learn from each node  \n",
    "*window_size*: Window size of skipgram model  \n",
    "*workers*: Number of parallel processes  \n",
    "\n",
    "**File/Directory parameters**  \n",
    "*output_filename*: Name for output representation file.  \n",
    "*item_file*: File path for the file that contains entity to entity relationships (e.g. wikibase-item).  \n",
    "*label_file*: File path for the file that contains wikidata labels.  \n",
    "*work_dir*: Path to work_dir that was specified in candidate_label_creation notebook. This should contain a folder called label_creation that has a type_mapping.tsv file within it.  \n",
    "*store_dir*: Path to folder containing the sqlite3.db file that we will use for our queries. We will reuse an existing file if there is one in this folder. Otherwise we will create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model params\n",
    "undirected = True\n",
    "number_walks = 10\n",
    "walk_length = 10\n",
    "seed = 0\n",
    "representation_size = 64\n",
    "window_size = 5\n",
    "workers = 32\n",
    "\n",
    "# File/Directory params\n",
    "output_filename = \"HAS_vec\"\n",
    "item_file = \"../../Q44/data/Q44.part.wikibase-item.tsv\"\n",
    "label_file = \"../../Q44/data/Q44.label.en.tsv\"\n",
    "work_dir = \"../../Q44/profiler_work_string_and_untrimmed_quantity\"\n",
    "store_dir = \"../../Q44\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process parameters and set up variables / file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure paths are absolute\n",
    "item_file = os.path.abspath(item_file)\n",
    "label_file = os.path.abspath(label_file)\n",
    "work_dir = os.path.abspath(work_dir)\n",
    "store_dir = os.path.abspath(store_dir)\n",
    "    \n",
    "# Create directories\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "output_dir = \"{}/HAS_embeddings\".format(work_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_file = \"{}/{}\".format(output_dir, output_filename)\n",
    "\n",
    "# Setting up environment variables \n",
    "os.environ['ITEM_FILE'] = item_file\n",
    "os.environ['LABEL_FILE'] = label_file\n",
    "os.environ['STORE'] = \"{}/wikidata.sqlite3.db\".format(store_dir)\n",
    "os.environ['OUT'] = output_dir\n",
    "os.environ['WORK'] = work_dir\n",
    "os.environ['kgtk'] = \"kgtk\" # Need to do this for kgtk to be recognized as a command when passing it through a subprocess call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H-Paths\n",
    "These random walks are intended to detect similarity due to homophily. Random walks are performed in a DFS manner\n",
    "\n",
    "First we need to create a file to use as input where each line has the format: entity1 entity2 entity1-type entity2-type\n",
    "\n",
    "# *do we want to use \"distinct\" here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove `%%capture` to see example of input file we are creating with human readable labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!kgtk query -i $WORK/label_creation/type_mapping.tsv -i $ITEM_FILE -i $LABEL_FILE --graph-cache $STORE \\\n",
    "--match '`'\"$ITEM_FILE\"'`: (e1)-[l]->(e2), type: (e1)-[]->(e1_type), type: (e2)-[]->(e2_type), `'\"$LABEL_FILE\"'`: (e1)-[:label]->(e1_lab), `'\"$LABEL_FILE\"'`: (e2)-[:label]->(e2_lab), `'\"$LABEL_FILE\"'`: (e1_type)-[:label]->(e1_type_lab), `'\"$LABEL_FILE\"'`: (e2_type)-[:label]->(e2_type_lab)' \\\n",
    "--return 'distinct e1 as entity1, e2 as entity2, e1_type as entity1_type, e2_type as entity2_type, count(distinct l) as count, e1_lab as e1_lab, e2_lab as e2_lab, e1_type_lab as e1_type_lab, e2_type_lab as e2_type_lab' \\\n",
    "--where 'e1_lab.kgtk_lqstring_lang_suffix = \"en\" AND e2_lab.kgtk_lqstring_lang_suffix = \"en\" AND e1_type_lab.kgtk_lqstring_lang_suffix = \"en\" AND e2_type_lab.kgtk_lqstring_lang_suffix = \"en\"' \\\n",
    "--order-by 'count(distinct l) desc' \\\n",
    "--limit 5 \\\n",
    "| column -t -s $'\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk query -i $WORK/label_creation/type_mapping.tsv -i $ITEM_FILE \\\n",
    "-o $OUT/h_file_in.tsv --graph-cache $STORE \\\n",
    "--match '`'\"$ITEM_FILE\"'`: (e1)-[]->(e2), type: (e1)-[]->(e1_type), type: (e2)-[]->(e2_type)' \\\n",
    "--return 'distinct e1 as entity1, e2 as entity2, e1_type as entity1_type, e2_type as entity2_type' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove header\n",
    "with open('{}/h_file_in.tsv'.format(output_dir), 'r') as fin:\n",
    "    data = fin.read().splitlines(True)\n",
    "with open('{}/h_file_in.tsv'.format(output_dir), 'w') as fout:\n",
    "    fout.writelines(data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q153546    Q1726  Q131734   Q1066984\r\n",
      "Q153546    Q1726  Q167270   Q1066984\r\n",
      "Q19237092  Q1726  Q191067   Q1066984\r\n",
      "Q1524      Q61    Q1549591  Q1093829\r\n",
      "Q1524      Q61    Q5119     Q1093829\r\n",
      "Q1524      Q61    Q515      Q1093829\r\n",
      "Q30        Q61    Q1489259  Q1093829\r\n",
      "Q30        Q61    Q1520223  Q1093829\r\n",
      "Q30        Q61    Q3624078  Q1093829\r\n",
      "Q30        Q61    Q43702    Q1093829\r\n"
     ]
    }
   ],
   "source": [
    "!head $OUT/h_file_in.tsv | column -t -s $'\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b4875949c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_edgelist_with_nodetype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/h_file_in.tsv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mundirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mundirected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m walks = graph.build_corpus(G, num_paths=number_walks,\n\u001b[1;32m      3\u001b[0m                      path_length=walk_length, alpha=0, rand=random.Random(seed))\n",
      "\u001b[0;32m~/Documents/grad_school/Research.nosync/kgtk/entity_profiling/src/graph.py\u001b[0m in \u001b[0;36mload_edgelist_with_nodetype\u001b[0;34m(file_, undirected)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtypelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select distinct(entity1_type_id) from relation_triples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_path' is not defined"
     ]
    }
   ],
   "source": [
    "G = graph.load_edgelist_with_nodetype('{}/h_file_in.tsv'.format(output_dir), undirected=undirected)\n",
    "walks = graph.build_corpus(G, num_paths=number_walks,\n",
    "                     path_length=walk_length, alpha=0, rand=random.Random(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info('H finished')\n",
    "\n",
    "f = open(file_S, \"r\")\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    m = line.strip().split(' ')\n",
    "    walks.append(m)\n",
    "logging.info('S finished')\n",
    "f.close()\n",
    "\n",
    "f = open(file_P, \"r\")\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    m = line.strip().split(' ')\n",
    "    walks.append(m)\n",
    "logging.info('P finished')\n",
    "f.close()\n",
    "\n",
    "model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, sg=1, hs=1,\n",
    "                 workers=workers)\n",
    "model.wv.save_word2vec_format(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kgtk-env] *",
   "language": "python",
   "name": "conda-env-kgtk-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
