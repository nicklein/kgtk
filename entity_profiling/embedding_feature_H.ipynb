{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H-Feature Analysis\n",
    "This is an optional notebook where we will go through the steps of creating random walks using the H-path method. These can be used as a feature when learning HAS-embeddings in the HAS_entity_embeddings notebook. We will do some analysis of what happens with different parameter choices as well as results of using the feature for learning embeddings.\n",
    "\n",
    "*What is this feature?* --> These random walks are intended to detect similarity due to homophily. I.e. entities with similar neighbors are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisite steps to run this notebook\n",
    "1. You need to run the 1_candidate_label_creation notebook before this notebook.\n",
    "2. gensim is a dependency. You can install it with `pip install --upgrade gensim`, or if you want to use Anaconda, `conda install -c conda-forge gensim`\n",
    "3. graph-tool is a dependency. Instructions to install in various ways are here: https://git.skewed.de/count0/graph-tool/-/wikis/installation-instructions (e.g. to install in an existing conda environment, use `conda install -c conda-forge graph-tool`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/nmklein/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import graph_tool.all as gt\n",
    "from kgtk.gt.gt_load import load_graph_from_kgtk\n",
    "from kgtk.io.kgtkreader import KgtkReader\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from h_path_walks import gt_random_walks_from_nodes\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters\n",
    "\n",
    "**Embedding model parameters**   \n",
    "*num_walks*: Number of random walks to start at each node with the H-feature walk method  \n",
    "*walk_length*: Length of random walk started at each node  \n",
    "*representation_size*: Number of latent dimensions to learn from each node  \n",
    "*window_size*: Window size of skipgram model  \n",
    "*workers*: Number of parallel processes  \n",
    "\n",
    "**File/Directory parameters**  \n",
    "*item_file*: File path for the file that contains entity to entity relationships (e.g. wikibase-item).  \n",
    "*label_file*: File path for the file that contains wikidata labels.  \n",
    "*work_dir*: same work_dir that you specified in the label creation notebook. Files created by this notebook will also be saved here.  \n",
    "*store_dir*: Path to folder containing the sqlite3.db file that we will use for our queries. We will reuse an existing file if there is one in this folder. Otherwise we will create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model params\n",
    "num_walks = 3\n",
    "walk_length = 6\n",
    "representation_size = 64\n",
    "window_size = 4\n",
    "workers = 12\n",
    "\n",
    "# File/Directory params\n",
    "item_file = \"./data/wikidata-20210215-dwd/claims.wikibase-item.tsv.gz\"\n",
    "label_file = \"./data/wikidata-20210215-dwd/labels.en.tsv.gz\"\n",
    "work_dir = \"./output/wikidata-20210215-dwd\"\n",
    "store_dir = \"./output/wikidata-20210215-dwd/temp-h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process parameters and set up variables / file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure paths are absolute\n",
    "item_file = os.path.abspath(item_file)\n",
    "label_file = os.path.abspath(label_file)\n",
    "work_dir = os.path.abspath(work_dir)\n",
    "store_dir = os.path.abspath(store_dir)\n",
    "    \n",
    "# Create directories\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "output_dir = \"{}/H_walks_analysis\".format(work_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(store_dir):\n",
    "    os.makedirs(store_dir)\n",
    "    \n",
    "# Number of nodes to compute walks from at once. Larger = faster, but if too big, we may run out of stack space.\n",
    "# Not making this a user-specifiable param for now.\n",
    "batch_size = 50000 #TODO move this inside h_path_walks.py\n",
    "\n",
    "# Names of files we will create\n",
    "directed_walks_file = \"{}/h_walks_directed_3x6.txt\".format(output_dir)\n",
    "undirected_walks_file = \"{}/h_walks_undirected_3x6.txt\".format(output_dir)\n",
    "# undirected_walks_file = \"{}/1M_walks.txt\".format(output_dir)\n",
    "\n",
    "# Setting up environment variables \n",
    "os.environ['ITEM_FILE'] = item_file\n",
    "os.environ['LABEL_FILE'] = label_file\n",
    "os.environ['STORE'] = \"{}/wikidata.sqlite3.db\".format(store_dir)\n",
    "os.environ['LABEL_CREATION'] = \"{}/label_creation\".format(work_dir)\n",
    "os.environ['OUT'] = output_dir\n",
    "os.environ['kgtk'] = \"kgtk\" # Need to do this for kgtk to be recognized as a command when passing it through a subprocess call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_walk_length_dist(walks):\n",
    "    print(\"Number of walks: {}\".format(len(walks)))\n",
    "    walk_lengths = [len(arr) for arr in walks]\n",
    "    print(\"Number of walks of each length:\")\n",
    "    counts_str = \", \".join([\"{} : {}\".format(key,value) for key,value in sorted(Counter(walk_lengths).items())])\n",
    "    print(counts_str)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(walk_lengths,bins=np.arange(12)-.5)\n",
    "    ax.set_ylabel('Number of walks')\n",
    "    ax.set_xlabel('Walk length')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_distinct_nodes_in_walks(walks):\n",
    "    print(\"Number of walks: {}\".format(len(walks)))\n",
    "    count_distinct_nodes = [len(set(arr)) for arr in walks]\n",
    "    print(\"Number of walks by number of unique nodes visited:\")\n",
    "    counts_str = \", \".join([\"{} : {}\".format(key,value) for key,value in sorted(Counter(count_distinct_nodes).items())])\n",
    "    print(counts_str)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(count_distinct_nodes,bins=np.arange(12)-.5)\n",
    "    ax.set_ylabel('Number of walks')\n",
    "    ax.set_xlabel('Number of unique nodes visited')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Directed graph representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load graph from item to item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kr = KgtkReader.open(pathlib.Path(item_file))\n",
    "g = load_graph_from_kgtk(kr, directed=True, hashed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph has 944403 vertices and 2402344 edges\n"
     ]
    }
   ],
   "source": [
    "print(\"This graph has {} vertices and {} edges\".format(len(g.get_vertices()), len(g.get_edges())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/nmklein/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with batch: 0\n",
      "done with batch: 1\n",
      "done with batch: 2\n",
      "done with batch: 3\n",
      "done with batch: 4\n",
      "done with batch: 5\n",
      "done with batch: 6\n",
      "done with batch: 7\n",
      "done with batch: 8\n",
      "done with batch: 9\n",
      "done with batch: 10\n",
      "done with batch: 11\n",
      "done with batch: 12\n",
      "done with batch: 13\n",
      "done with batch: 14\n",
      "done with batch: 15\n",
      "done with batch: 16\n",
      "done with batch: 17\n",
      "done with batch: 18\n",
      "CPU times: user 13min 20s, sys: 14 s, total: 13min 34s\n",
      "Wall time: 13min 35s\n"
     ]
    }
   ],
   "source": [
    "open(directed_walks_file, 'w').close()\n",
    "count_walk_lens = np.zeros(walk_length+1, dtype=int)\n",
    "vertices = g.get_vertices()\n",
    "num_batches = int(np.ceil(len(vertices) / batch_size))\n",
    "print(\"num_batches: {}\".format(num_batches))\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    start_nodes = vertices[batch_num*batch_size : (batch_num+1)*batch_size]\n",
    "    h_walks_directed = gt_random_walks_from_nodes(g, start_nodes, walk_length, num_walks)\n",
    "    \n",
    "    # keep track of stats\n",
    "    for walk in h_walks_directed:\n",
    "        count_walk_lens[len(walk)] += 1\n",
    "        \n",
    "    # Explicitly cast list of lists to ndarray with dtype=object to avoid ragged nested sequences message\n",
    "    h_walks_directed = np.array(h_walks_directed, dtype=object)\n",
    "    with open(directed_walks_file, \"a\") as f:\n",
    "        np.savetxt(f, h_walks_directed, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many walks we have and how long they are (max walk length is 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of walks: 9444030.0\n",
      "Number of walks of each length:\n",
      "0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 9444030.0\n"
     ]
    }
   ],
   "source": [
    "print(\"number of walks: {}\".format(sum(count_walk_lens)))\n",
    "counts_str = \", \".join([\"{}: {}\".format(i, count_walk_lens[i]) for i in range(len(count_walk_lens))])\n",
    "print(\"Number of walks of each length:\\n{}\".format(counts_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Undirected representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load graph from item to item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr = KgtkReader.open(pathlib.Path(item_file))\n",
    "g = load_graph_from_kgtk(kr, directed=False, hashed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph has 42575933 vertices and 182246240 edges\n"
     ]
    }
   ],
   "source": [
    "print(\"This graph has {} vertices and {} edges\".format(len(g.get_vertices()), len(g.get_edges())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/852 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches: 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 852/852 [1:53:52<00:00,  8.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 44min 25s, sys: 9min 31s, total: 1h 53min 57s\n",
      "Wall time: 1h 53min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "open(undirected_walks_file, 'w').close()\n",
    "count_walk_lens = np.zeros(walk_length+1, dtype=int)\n",
    "count_unique_nodes_in_walk = np.zeros(walk_length+1, dtype=int)\n",
    "vertices = g.get_vertices()\n",
    "num_batches = int(np.ceil(len(vertices) / batch_size))\n",
    "print(\"num_batches: {}\".format(num_batches))\n",
    "with open(undirected_walks_file, \"a\") as f:\n",
    "    for batch_num in tqdm(range(num_batches)):\n",
    "        start_nodes = vertices[batch_num*batch_size : (batch_num+1)*batch_size]\n",
    "        h_walks_undirected = gt_random_walks_from_nodes(g, start_nodes, walk_length, num_walks)\n",
    "        # keep track of stats\n",
    "        for walk in h_walks_undirected:\n",
    "            count_walk_lens[len(walk)] += 1\n",
    "            count_unique_nodes_in_walk[len(set(walk))] += 1\n",
    "        np.savetxt(f, h_walks_undirected, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of walks: 127727799\n",
      "Number of walks of each length:\n",
      "0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 127727799\n"
     ]
    }
   ],
   "source": [
    "print(\"number of walks: {}\".format(sum(count_walk_lens)))\n",
    "counts_str = \", \".join([\"{}: {}\".format(i, count_walk_lens[i]) for i in range(len(count_walk_lens))])\n",
    "print(\"Number of walks of each length:\\n{}\".format(counts_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these walks are on an undirected graph, it might be informative to look at how many unique vertexes we visit in each walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of walks by number of unique nodes visited:\n",
      "0: 0, 1: 422, 2: 125154, 3: 213616, 4: 540475, 5: 1491401, 6: 37087106, 7: 45449188, 8: 104035468, 9: 142833307, 10: 93983193\n"
     ]
    }
   ],
   "source": [
    "counts_str = \", \".join([\"{}: {}\".format(i, count_unique_nodes_in_walk[i]) for i in range(len(count_unique_nodes_in_walk))])\n",
    "print(\"Number of walks by number of unique nodes visited:\\n{}\".format(counts_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let's see what embeddings we learn if we only use this feature\n",
    "Use Skip-Gram model to learn representations for the entities  \n",
    "**We'll use the undirected representation's h-path walks.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "undirected_walks_file_1 = '/data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1.txt'\n",
    "undirected_walks_file_1_small = '/data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small.txt'\n",
    "undirected_walks_file_1_small2 = '/data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42575933 /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -50000 /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small.txt \\\n",
    ">> /data/profiling/kgtk/entity_profiling/output/wikidata-20210215-dwd/H_walks_analysis/h_walks_undirected_1_small2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try to empirically find a good value for min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127727799it [07:16, 292452.43it/s]\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "with open(undirected_walks_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42575933/42575933 [00:15<00:00, 2766071.97it/s]\n"
     ]
    }
   ],
   "source": [
    "rare_words = {}\n",
    "for word, count in tqdm(word_counts.items()):\n",
    "    if count not in rare_words:\n",
    "        rare_words[count] = []\n",
    "    rare_words[count].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42575933"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8951099"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(words) for count, words in rare_words.items() if count <= 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050204"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_words[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q18196617'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(rare_words[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[\"Q10598940\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In original 10-walks per entity file:  \n",
    "42.6M entities  \n",
    "\n",
    "| num ocurrences | num entities with <= this num ocurrences | examples of entities with this num ocurrences |  \n",
    "| :- | :- | :- |  \n",
    "| <=50 | 22,912,461 | Neotriozella pyrifolii (species of insect), Q13785750 (no english label), ð’…… (unicode character), Sungai Lingit (disambig page), Thereva brunnea (species of insect) |\n",
    "| <=40 | 12,238,525 | |\n",
    "| <=30 | 4,192,879 | |\n",
    "| <=20 | 683,155 | |\n",
    "| <=10 | 164 | |\n",
    "\n",
    "num ocurrences of various entities:\n",
    "\n",
    "| entity | Qnode | frequency |\n",
    "| :- | :- | :- |\n",
    "| Political career of Vladimir Putin | Q17052997 | 35 |\n",
    "| Putin's presidential campaign | Q45023984 | 22 |\n",
    "| Putin disamb. page | Q4384355 | 73 |\n",
    "| Putin's father | Q19300851 | 223 |\n",
    "| Medvedev (russian pol) | Q23530 | 884 |\n",
    "| Medvedev (rus. biologist) | Q2096791 | 211 |\n",
    "| Medvedev (rus. librettist (opera)) | Q27230041 | 120 |\n",
    "| Yevgeniy Ditrikh (russian politician) | Q53803011 | 75 |\n",
    "| Pedro Szekely | Q100104271 | 100 |\n",
    "| Memento (movie) | Q190525 | 348 |\n",
    "| Saint Abanoub | Q2562070 | 69 |\n",
    "| Saint Barbara | Q192816 | 6109 |\n",
    "| Legend of Zelda Twilight Princess | Q735613 | 284 |\n",
    "| Gone Girl (book) | Q5581570 | 117 |\n",
    "\n",
    "\n",
    "50 looks like a reasonable cutoff.\n",
    "\n",
    "testing run-time...\n",
    "\n",
    "In 1-walk per entity file:  \n",
    "<=5: 25M  \n",
    "<=4: 19M  \n",
    "<=3: 12M  \n",
    "<=2: 6M  \n",
    "<=1: 1.8M  \n",
    "\n",
    "In trimmed 1-walk per entity file that has 100,000 walks + 50,000 duplicates:  \n",
    "550k entities  \n",
    "1:266k  \n",
    "*Runtime with min_count=0*: 1 min for setup and 19s per epoch  \n",
    "*Runtime with min_count=2*: <1 min for setup and 13s per epoch  \n",
    "\n",
    "Now finding appropriate min_count for 3 walks from each entity with walk length 6...  \n",
    "\n",
    "| num ocurrences | num entities with <= this num ocurrences | examples of entities with this num ocurrences |  \n",
    "| :- | :- | :- | \n",
    "| <=10 | 27.6M | |\n",
    "| <=9 | 23.6M | |\n",
    "| <=8 | 18.8M | Category:Fish described in 1884, Category:Amur Shipbuilding Plant, Q21709829 (no label), Alexander Wedl (German ice hockey player) |\n",
    "| <=7 | 13.8M | Atanas Babata (bulgarian revolutionary) |\n",
    "| <=6 | 9M | |\n",
    "\n",
    "num ocurrences of various entities:\n",
    "\n",
    "| entity | Qnode | frequency |\n",
    "| :- | :- | :- |\n",
    "| Political career of Vladimir Putin | Q17052997 | 7 |\n",
    "| Putin's presidential campaign | Q45023984 | 5 |\n",
    "| Putin disamb. page | Q4384355 | 11 |\n",
    "| Putin's father | Q19300851 | 36 |\n",
    "| Medvedev (russian pol) | Q23530 | 109 |\n",
    "| Medvedev (rus. biologist) | Q2096791 | 27 |\n",
    "| Medvedev (rus. librettist (opera)) | Q27230041 | 16 |\n",
    "| Yevgeniy Ditrikh (russian politician) | Q53803011 | 11 |\n",
    "| Pedro Szekely | Q100104271 | 11 |\n",
    "| Memento (movie) | Q190525 | 43 |\n",
    "| Saint Abanoub | Q2562070 | 10 |\n",
    "| Saint Barbara | Q192816 | 1000 |\n",
    "| Legend of Zelda Twilight Princess | Q735613 | 34 |\n",
    "| Gone Girl (book) | Q5581570 | 21 |\n",
    "\n",
    "Since the margins are smaller here I'll play it a bit safer and go with min_count=8 (ignores 13.8M entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2Vec process @ 2021-05-07 01:15:55.082173\n",
      "Epoch #0 start -- 2021-05-07 01:53:59.064302\n",
      "Epoch #0 end -- 2021-05-07 03:35:43.243992 -- elapsed time: 1:41:44.179690\n",
      "Epoch #1 start -- 2021-05-07 03:35:43.244273\n",
      "Epoch #1 end -- 2021-05-07 05:18:51.101256 -- elapsed time: 1:43:07.856983\n",
      "Epoch #2 start -- 2021-05-07 05:18:51.101523\n",
      "Epoch #2 end -- 2021-05-07 07:01:54.190864 -- elapsed time: 1:43:03.089341\n",
      "Epoch #3 start -- 2021-05-07 07:01:54.191152\n",
      "Epoch #3 end -- 2021-05-07 08:44:34.370598 -- elapsed time: 1:42:40.179446\n",
      "Epoch #4 start -- 2021-05-07 08:44:34.370849\n",
      "Epoch #4 end -- 2021-05-07 10:25:27.396399 -- elapsed time: 1:40:53.025550\n",
      "Now saving model...\n",
      "Now saving keyed vectors...\n",
      "CPU times: user 3d 20h 43min 40s, sys: 2h 35min 52s, total: 3d 23h 19min 32s\n",
      "Wall time: 9h 21min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        self.epoch_start = datetime.now()\n",
    "        print(\"Epoch #{} start -- {}\".format(self.epoch, str(self.epoch_start)))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        epoch_end = datetime.now()\n",
    "        time_elapsed = str(epoch_end - self.epoch_start)\n",
    "        print(\"Epoch #{} end -- {} -- elapsed time: {}\".format(self.epoch, str(epoch_end), time_elapsed))\n",
    "        self.epoch += 1\n",
    "\n",
    "print(\"Starting Word2Vec process @ {}\".format(datetime.now()))\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "model = Word2Vec(corpus_file=undirected_walks_file, vector_size=representation_size,\n",
    "                 window=window_size, min_count=8, sg=1, hs=1, workers=workers, callbacks=[epoch_logger])\n",
    "\n",
    "print(\"Now saving model...\")\n",
    "model.save(\"{}/h_embeddings_3x6,min_count=8.model\".format(output_dir))\n",
    "\n",
    "print(\"Now saving keyed vectors...\")\n",
    "model.wv.save(\"{}/h_embeddings_3x6,min_count=8.kv\".format(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the embeddings\n",
    "We want similar entities to have more similar embeddings. For the purpose of profiling entities of a desired type, we are specifically interested in similar entities *within a type* having more similar embeddings. We'll investigate using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare entities of different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"{}/h_embeddings_3x6,min_count=8.kv\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vladimir Putin and Russia: 0.64\n",
      "Vladimir Putin and Ireland: 0.11\n",
      "Russia and Ireland: 0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"Vladimir Putin and Russia: {:.2f}\".format(wv.similarity('Q7747','Q159')))\n",
    "print(\"Vladimir Putin and Ireland: {:.2f}\".format(wv.similarity('Q7747','Q27')))\n",
    "print(\"Russia and Ireland: {:.2f}\".format(wv.similarity('Q159','Q27')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare various entities within the type we are interested in profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('Q7747')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Q28858481' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2f4dc7e40084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vladimir Putin and : {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Q7747'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q28858481'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kgtk-env/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Q28858481' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(\"Vladimir Putin and : {:.2f}\".format(wv.similarity('Q7747','Q28858481')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at how similar each beer is to eachother (since we have very few of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_vecs = model.wv['Q12877510', 'Q61976614', 'Q93552342', 'Q93557205', 'Q93559285', 'Q93558270', 'Q93560567', 'Q97412285']\n",
    "similarity_mat = [model.wv.cosine_similarities(beer, beer_vecs) for beer in beer_vecs]\n",
    "mask = np.zeros_like(similarity_mat)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "labels = ['Macedonian Thrace Brewery', 'Rastrum', 'Vergina Lager', 'Vergina Red', 'Vergina Weiss', 'Vergina Porfyra', 'Vergina Black', 'Vergina Alcohol Free']\n",
    "# Could mask to only show the lower triangle, but I think this is actually easier to read without the mask\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(similarity_mat, ax=ax, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "plt.xticks(rotation=30, horizontalalignment='right')\n",
    "plt.title(\"Cosine similarity of beer embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beer called 'Rastrum' seems to be an outlier. Searching for it online gives few results related to beer. Looking at similarities without this beer so we can more easily see how the others compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_vecs = model.wv['Q12877510', 'Q93552342', 'Q93557205', 'Q93559285', 'Q93558270', 'Q93560567', 'Q97412285']\n",
    "similarity_mat = [model.wv.cosine_similarities(beer, beer_vecs) for beer in beer_vecs]\n",
    "mask = np.zeros_like(similarity_mat)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "labels = ['Macedonian Thrace Brewery', 'Vergina Lager', 'Vergina Red', 'Vergina Weiss', 'Vergina Porfyra', 'Vergina Black', 'Vergina Alcohol Free']\n",
    "# Could mask to only show the lower triangle, but I think this is actually easier to read without the mask\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(similarity_mat, ax=ax, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "plt.xticks(rotation=30, horizontalalignment='right')\n",
    "plt.title(\"Cosine similarity of beer embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
